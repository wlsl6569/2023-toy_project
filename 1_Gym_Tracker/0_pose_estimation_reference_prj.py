# -*- coding: utf-8 -*-
"""pose_estimation (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRGvD2p7AN0IU7Zpq9ApPdiNm_rngw2q

# 0.Install and Import Dependencies
"""

!pip install mediapipe opencv-python

"""### 메인 도구 임포트"""

import cv2 # open cv
import mediapipe as mp 
import numpy as np
mp_drawing = mp.solutions.drawing_utils # drawing utilities - 포즈 시각화할 때 사용
mp_pose = mp.solutions.pose # pose 관련 model 임포트

"""### 피드 만들기 - 기본

"""

import cv2

# 기본 카메라(인덱스 0)를 열기 위해 VideoCapture 객체를 생성
cap = cv2.VideoCapture(0)

# 카메라가 열릴 때까지 반복
while cap.isOpened():
    # 카메라에서 프레임을 읽기, ret은 성공 여부
    ret, frame = cap.read()

    # "Mediapipe Feed"라는 이름의 창에 프레임을 표시
    cv2.imshow('Mediapipe Feed', frame)
    
    # 키 입력을 대기하고 눌린 키가 'q'인지 확인. 'q'를 누르면 종료
    if cv2.waitKey(10) & 0xFF == ord('q'):
        break

# 카메라를 해제하고 모든 창을 닫기
cap.release()
cv2.destroyAllWindows()

"""# 1. Detection 만들기
실제로 관절 위에 visualizing된 joint 객체 생성
"""

import cv2
import mediapipe as mp

# 웹캠 비디오 캡처를 위한 VideoCapture 객체 생성
cap = cv2.VideoCapture(0)

# Mediapipe의 Pose 모델 인스턴스 생성
with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
    # 웹캠이 열려있는 동안 반복 실행
    while cap.isOpened():
        # 비디오 프레임 읽기
        ret, frame = cap.read()
        
        # 프레임을 RGB 색상 공간으로 변환
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        
        # Mediapipe Pose 모델을 이용하여 프레임 처리
        results = pose.process(image)
        
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # 검출된 포즈 랜드마크를 프레임에 그리기 (이미지, 조인트, 조인트커넥션)이 변수로 들어감
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),# joint 스타일
                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=5) # joint line 스타일                      
                                 )
        
        # 처리된 프레임을 화면에 출력
        cv2.imshow('Mediapipe Feed', image)
        
        # 'q' 키를 누르면 반복 종료
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
    
    # 비디오 캡처 객체와 창 닫기
    cap.release()
    cv2.destroyAllWindows()

"""# 2. joint 추적
랜드마크(조인트 위치)  추출

"""

import cv2
import mediapipe as mp

# 웹캠 비디오 캡처를 위한 VideoCapture 객체 생성
cap = cv2.VideoCapture(0)

# Mediapipe의 Pose 모델 인스턴스 생성
with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
    # 웹캠이 열려있는 동안 반복 실행
    while cap.isOpened():
        # 비디오 프레임 읽기
        ret, frame = cap.read()
        
        # 프레임을 RGB 색상 공간으로 변환
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        
        # Mediapipe Pose 모델을 이용하여 프레임 처리
        results = pose.process(image)
        
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # 랜드 마크 추출
        try:  # 가끔은 랜드마크가 나오지않을 때도 있기 때문에 try - expect를 통해 랜드마크를 추출하는 데 실패하더라도 계속 진행하게 함   
            landmarks = results.pose_landmarks.landmark
            print(landmarks)
        except:
            pass
        
        
        # 검출된 포즈 랜드마크를 프레임에 그리기 (이미지, 조인트, 조인트커넥션)이 변수로 들어감
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),# joint 스타일
                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=5) # joint line 스타일                      
                                 )
        
        # 처리된 프레임을 화면에 출력
        cv2.imshow('Mediapipe Feed', image)
        
        # 'q' 키를 누르면 반복 종료
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
    
    # 비디오 캡처 객체와 창 닫기
    cap.release()
    cv2.destroyAllWindows()

len(landmarks) # 0번 부터 32번까지의 랜드마크 총 갯수

"""###  특정 랜드마크를 선택하기"""

# 랜드마크 순차대로 보기

for lndmrk in mp_pose.PoseLandmark:
    print(lndmrk)

# 랜드마크 (관절)의 위치값 보기

print(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value])

# 랜드마크의 번호 보기

print(mp_pose.PoseLandmark.LEFT_SHOULDER)

# 예시용 팔 운동에 사용될 랜드마크(왼쪽 어깨 - 왼쪽 팔꿈치- 왼쪽 손목) 값 불러오기 

print(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value])
print(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value])
print(landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])

"""# 3.특정 조인트 간의 각도 계산"""

def calculate_angle(a, b, c):  # 세 개의 랜드마크 사이의 각도 계산
    a = np.array(a) 
    b = np.array(b) 
    c = np.array(c) 

    # 각도 계산
    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(b[1] - a[1], b[0] - a[0])
    angle = np.abs(radians*180.0/np.pi)
    
    # 팔의 최대 각도 
    
    if angle > 180.0:
        angle = 360-angle 
        
    return angle

# calculate_angle 함수에 들어갈 변수 지정 : 각각의 관절 x,y 위치값이 포함됨
shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]

calculate_angle(shoulder,elbow,wrist)

# 각도를 시각화하여 팔운동 curl 코드 얻을 수 있게 하기

# 웹캠 비디오 캡처를 위한 VideoCapture 객체 생성
cap = cv2.VideoCapture(0)

# Mediapipe의 Pose 모델 인스턴스 생성
with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
    # 웹캠이 열려있는 동안 반복 실행
    while cap.isOpened():
        # 비디오 프레임 읽기
        ret, frame = cap.read()
        
        # 프레임을 RGB 색상 공간으로 변환
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        
        # Mediapipe Pose 모델을 이용하여 프레임 처리
        results = pose.process(image)
        
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # 랜드 마크 추출
        try:  # 가끔은 랜드마크가 나오지않을 때도 있기 때문에 try - expect를 통해 랜드마크를 추출하는 데 실패하더라도 계속 진행하게 함   
            landmarks = results.pose_landmarks.landmark
            
            # 변수 지정
            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]
            
            # 각도계산
            angle = calculate_angle(shoulder, elbow, wrist)
            
            # 시각화
            cv2.putText(image, str(angle), 
                       tuple(np.multiply(elbow, [640,480]).astype(int)), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA
                       )

            
            print(landmarks)
        except:
            pass
        
        
        # 검출된 포즈 랜드마크를 프레임에 그리기 (이미지, 조인트, 조인트커넥션)이 변수로 들어감
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),# joint 스타일
                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=5) # joint line 스타일                      
                                 )
        
        # 처리된 프레임을 화면에 출력
        cv2.imshow('Mediapipe Feed', image)
        
        # 'q' 키를 누르면 반복 종료
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
    
    # 비디오 캡처 객체와 창 닫기
    cap.release()
    cv2.destroyAllWindows()

"""# 4.특정 각도가 이상을 넘으면 카운트하기"""

# 웹캠 비디오 캡처를 위한 VideoCapture 객체 생성
cap = cv2.VideoCapture(0)

# curl counter 변수지정
counter = 0
stage = None # 관절이 꺾이는 방향 (위쪽//아래쪽) 인지

# Mediapipe의 Pose 모델 인스턴스 생성
with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
    # 웹캠이 열려있는 동안 반복 실행
    while cap.isOpened():
        # 비디오 프레임 읽기
        ret, frame = cap.read()
        
        # 프레임을 RGB 색상 공간으로 변환
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        
        # Mediapipe Pose 모델을 이용하여 프레임 처리
        results = pose.process(image)
        
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # 랜드 마크 추출
        try:  # 가끔은 랜드마크가 나오지않을 때도 있기 때문에 try - expect를 통해 랜드마크를 추출하는 데 실패하더라도 계속 진행하게 함   
            landmarks = results.pose_landmarks.landmark
            
            # 변수 지정
            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]
            
            # 각도계산
            angle = calculate_angle(shoulder, elbow, wrist)
            
            # 시각화
            cv2.putText(image, str(angle), 
                       tuple(np.multiply(elbow, [640,480]).astype(int)), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA
                       )
            
            # culr counter 구현 - 팔운동 상태 체크
            if angle > 160:
                stage = "down"   # 팔이 내려간 상태
            if angle < 30 and stage =='down': # 내려갔다가 팔이 다시 올라온 상태
                stage="up"
                counter +=1
                print(counter) # 출력창 확인

            
           #print(landmarks)
        except:
            pass
        
        # Render curl counter
        # statue box 설정
        cv2.rectangle(image, (0,0), (255,73), (245,117,16), -1) # 왼쪽 상단 모서리에 statue box 생성(이미지, 박스 시작점, 끝점, 컬러, 선너비-1로 지정하여 색으로 채워진 박스 만듦)
        
        # REPS 내용
        cv2.putText(image, 'REPS', (15,12), # 운동에서 동작의 반복 횟수(reps) 확인, 좌표:15,12, 그 외 글꼴 설정
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)
        cv2.putText(image, str(counter), (10,65),
                   cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 1, cv2.LINE_AA)
        
        # stage data 내용
        cv2.putText(image, 'STAGE', (65,12), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)
        cv2.putText(image, stage, (60,60),
                   cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 1, cv2.LINE_AA)
        
        
        
        # 검출된 포즈 랜드마크를 프레임에 그리기 (이미지, 조인트, 조인트커넥션)이 변수로 들어감
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),# joint 스타일
                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=5) # joint line 스타일                      
                                 )
        
        # 처리된 프레임을 화면에 출력
        cv2.imshow('Mediapipe Feed', image)
        
        # 'q' 키를 누르면 반복 종료
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
    
    # 비디오 캡처 객체와 창 닫기
    cap.release()
    cv2.destroyAllWindows()